{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Part 4.3: Sentiment Analysis (more about vaderSentiment)\n",
    "\n",
    "Changes made to vaderSentiment as follows, the accuracy score (NLTKCorpos_Twitter.csv) improves from 92.30% to 92.39%.\n",
    "\n",
    "- Add more words from LIWC Internal Dictionary 2015 (posemo & negemo):  'fabulously': 2.4,\n",
    "    'amazingly': 2.8\n",
    "    'fairer': 1.3\n",
    "    'fairest': 1.3\n",
    "    'fantastically': 2.0\n",
    "    'fantasy': 2.6\n",
    "    'kindn': 2.2\n",
    "    'yum': 2.4\n",
    "    'benefitt': 1.6\n",
    "    'bestest': 3.2\n",
    "    'bestie': 3.2\n",
    "    'besties': 3.2\n",
    "    'finer': 0.8\n",
    "    'finest': 0.8\n",
    "    'fortunately': 1.9\n",
    "    'free-think': 1.0\n",
    "    'freethink': 1.0\n",
    "    'lovelier': 2.8\n",
    "    'loveliest': 2.8\n",
    "    'grande': 1.0\n",
    "    'greatness': 3.2\n",
    "    'okey': 0.9\n",
    "    'peacekeep': 1.6\n",
    "    'peacemak': 2.0\n",
    "    'sweeter': 2.0\n",
    "    'sweetest': 2.0\n",
    "    'tender': 0.5\n",
    "    'thanking': 1.8\n",
    "    'thanx': 1.9\n",
    "    'thnx': 1.9\n",
    "    'heartwarm': 2.1\n",
    "    \"hero's\": 2.6\n",
    "    'enemie': -2.2\n",
    "    'ineffect': -1.3\n",
    "    'poorly': -2.1\n",
    "    'poorness': -2.1\n",
    "    'uncontrol': -1.5\n",
    "    'asshole': -2.8\n",
    "    'fuckh': -2.5\n",
    "    'fuckin': -2.5\n",
    "    'fucktwat': -3.1\n",
    "    'fuckwad': -3.1\n",
    "    'messier': -1.5\n",
    "    'scarier': -2.2\n",
    "    'scariest': -2.2\n",
    "    'deceptive':-1.9\n",
    "    'defend': -0.2\n",
    "    'harmful': -2.6\n",
    "    'messier': -1.5\n",
    "    'messiest': -1.5\n",
    "    'strangest': -0.8\n",
    "    'wrongdoing': -1.9\n",
    "    'wrongful': -1.9\n",
    "    'wrongly': -1.9\n",
    "    'wrongness;': -1.9\n",
    "    'wrongs': -1.9\n",
    "    'heartbroke': -2.7\n",
    "    'petrif': -0.3\n",
    "    'traged': -2.7\n",
    "    \n",
    "    \n",
    "- Add more negates: \"can't've\", \"couldn't've\", \"hadn't've\", \"mayn't\", \"maynot\", \"mightn't've\", \"mustn't've\", \"needn't've\",   \"sha'n't\", \"shan't've\", \"shouldn't've\", \"won't've\", \"wouldn't've\". \n",
    "\n",
    "\n",
    "- Adhere the following words: \"is not\": \"ain't\", \"are not\": \"aren't\",\"cannot\": \"can't\", \"can not\": \"can't\", \"cannot have\": \"can't've\", \"could not\": \"couldn't\", \"could not have\": \"couldn't've\",\"did not\": \"didn't\", \"does not\": \"doesn't\", \"do not\": \"don't\", \"had not\": \"hadn't\", \"had not have\": \"hadn't've\", \"has not\": \"hasn't\", \"have not\": \"haven't\", \"is not\": \"isn't\", \"may not\": \"mayn't\", \"might not\": \"mightn't\",\"might not have\": \"mightn't've\", \"must have\": \"must've\", \"must not\": \"mustn't\", \"must not have\": \"mustn't've\", \"need not\": \"needn't\", \"need not have\": \"needn't've\", \"ought not\": \"oughtn't\", \"ought not have\": \"oughtn't've\", \"shall not\": \"shan't\", \"shall not\": \"sha'n't\", \"shall not have\": \"shan't've\", \"should not\": \"shouldn't\", \"should not have\": \"shouldn't've\", \"was not\": \"wasn't\", \"were not\": \"weren't\", \"will not\": \"won't\", \"will not have\": \"won't've\",  \"would not\": \"wouldn't\", \"would not have\": \"wouldn't've\", \"no prblme\": \"noproblem\", \"no prob\": \"noprob\", \"no problems\": \"noproblmes\", \"no probs\": \"noprobs\", \"no worry\": \"noworry\", \"no worries\": \"noworries\", \"good bey\": \"goodbey\", \"long time no see\": \"longtimenosee\", \"nothing but\": \"nothingbut\", \"no more\": \"nomore\",\"kind of\": \"kindof\", \"sort of\": \"sortof\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "import requests\n",
    "import json\n",
    "from itertools import product\n",
    "from inspect import getsourcefile\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "os.chdir('C:\\\\Users\\\\wmj51\\\\Desktop\\\\python')\n",
    "\n",
    "from io import open\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Constants##\n",
    "\n",
    "# (empirically derived mean sentiment intensity rating increase for booster words)\n",
    "B_INCR = 0.293\n",
    "B_DECR = -0.293\n",
    "\n",
    "# booster/dampener 'intensifiers' or 'degree adverbs'\n",
    "# http://en.wiktionary.org/wiki/Category:English_degree_adverbs\n",
    "\n",
    "BOOSTER_DICT = \\\n",
    "    {\"absolutely\": B_INCR, \"amazingly\": B_INCR, \"awfully\": B_INCR, \"completely\": B_INCR, \"considerably\": B_INCR,\n",
    "     \"decidedly\": B_INCR, \"deeply\": B_INCR, \"effing\": B_INCR, \"enormously\": B_INCR,\n",
    "     \"entirely\": B_INCR, \"especially\": B_INCR, \"exceptionally\": B_INCR, \"extremely\": B_INCR,\n",
    "     \"fabulously\": B_INCR, \"flipping\": B_INCR, \"flippin\": B_INCR,\n",
    "     \"fricking\": B_INCR, \"frickin\": B_INCR, \"frigging\": B_INCR, \"friggin\": B_INCR, \"fully\": B_INCR, \"fucking\": B_INCR,\n",
    "     \"greatly\": B_INCR, \"hella\": B_INCR, \"highly\": B_INCR, \"hugely\": B_INCR, \"incredibly\": B_INCR,\n",
    "     \"intensely\": B_INCR, \"majorly\": B_INCR, \"more\": B_INCR, \"most\": B_INCR, \"particularly\": B_INCR,\n",
    "     \"purely\": B_INCR, \"quite\": B_INCR, \"really\": B_INCR, \"remarkably\": B_INCR,\n",
    "     \"so\": B_INCR, \"substantially\": B_INCR,\n",
    "     \"thoroughly\": B_INCR, \"totally\": B_INCR, \"tremendously\": B_INCR,\n",
    "     \"uber\": B_INCR, \"unbelievably\": B_INCR, \"unusually\": B_INCR, \"utterly\": B_INCR,\n",
    "     \"very\": B_INCR,\n",
    "     \"almost\": B_DECR, \"barely\": B_DECR, \"hardly\": B_DECR, \"just enough\": B_DECR,\n",
    "     \"kinda\": B_DECR, \"kindof\": B_DECR, \n",
    "     \"less\": B_DECR, \"little\": B_DECR, \"marginally\": B_DECR, \"occasionally\": B_DECR, \"partly\": B_DECR,\n",
    "     \"scarcely\": B_DECR, \"slightly\": B_DECR, \"somewhat\": B_DECR,\n",
    "     \"sorta\": B_DECR, \"sortof\": B_DECR, \"sort-of\": B_DECR}\n",
    "\n",
    "\n",
    "# (empirically derived mean sentiment intensity rating increase for using ALLCAPs to emphasize a word)\n",
    "C_INCR = 0.733\n",
    "N_SCALAR = -0.74\n",
    "\n",
    "# for removing punctuation\n",
    "REGEX_REMOVE_PUNCTUATION = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "PUNC_LIST = [\".\", \"!\", \"?\", \",\", \";\", \":\", \"-\", \"'\", \"\\\"\",\n",
    "             \"!!\", \"!!!\", \"??\", \"???\", \"?!?\", \"!?!\", \"?!?!\", \"!?!?\"]\n",
    "\n",
    "NEGATE = \\\n",
    "    [\"aint\", \"arent\", \"cannot\", \"cant\", \"couldnt\", \"darent\", \"didnt\", \"doesnt\",\n",
    "     \"ain't\", \"aren't\", \"can't\", \"couldn't\", \"daren't\", \"didn't\", \"doesn't\",\n",
    "     \"dont\", \"hadnt\", \"hasnt\", \"havent\", \"isnt\", \"mightnt\", \"mustnt\", \"neither\",\n",
    "     \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\",\n",
    "     \"neednt\", \"needn't\", \"never\", \"none\", \"nope\", \"nor\", \"not\", \"nothing\", \"nowhere\", \n",
    "     \"no\",\"nobody\", \"nomore\",\n",
    "     \"oughtnt\", \"shant\", \"shouldnt\", \"uhuh\", \"wasnt\", \"werent\", \n",
    "     \"oughtn't\", \"shan't\", \"shouldn't\", \"uh-uh\", \"wasn't\", \"weren't\",\n",
    "     \"without\", \"wont\", \"wouldnt\", \"won't\", \"wouldn't\", \"rarely\", \"seldom\", \"despite\",\n",
    "     # add more negates\n",
    "     \"can't've\", \"couldn't've\", \"hadn't've\", \"mayn't\", \"maynot\", \"mightn't've\", \"mustn't've\",\n",
    "     \"needn't've\", \"sha'n't\", \"shan't've\", \"shouldn't've\", \"won't've\", \"wouldn't've\", \n",
    "     \"idk\", \"must'nt\", \"need'nt\", \"noes\", \"nobod\", \"np\", \"ought'nt\", \"should'nt\"]\n",
    "    \n",
    "# check for sentiment laden idioms that do not contain lexicon words (future work, not yet implemented)\n",
    "SENTIMENT_LADEN_IDIOMS = {\"cut the mustard\": 2, \"hand to mouth\": -2,\n",
    "                          \"back handed\": -2, \"blow smoke\": -2, \"blowing smoke\": -2,\n",
    "                          \"upper hand\": 1, \"break a leg\": 2, \"no problem\": 2,\n",
    "                          \"cooking with gas\": 2, \"in the black\": 2, \"in the red\": -2,\n",
    "                          \"on the ball\": 2, \"under the weather\": -2}\n",
    "\n",
    "# check for special case idioms containing lexicon words\n",
    "SPECIAL_CASE_IDIOMS = {\"the shit\": 3, \"the bomb\": 3, \"bad ass\": 1.5, \"yeah right\": -2, \n",
    "                       \"kiss of death\": -1.5}\n",
    "\n",
    "\n",
    "# #Static methods# #\n",
    "\n",
    "def negated(input_words, include_nt=True):\n",
    "    \"\"\"\n",
    "    Determine if input contains negation words\n",
    "    \"\"\"\n",
    "    input_words = [str(w).lower() for w in input_words]\n",
    "    neg_words = []\n",
    "    neg_words.extend(NEGATE)\n",
    "    for word in neg_words:\n",
    "        if word in input_words:\n",
    "            return True\n",
    "    if include_nt:\n",
    "        for word in input_words:\n",
    "            if \"n't\" in word:\n",
    "                return True\n",
    "    if \"least\" in input_words:\n",
    "        i = input_words.index(\"least\")\n",
    "        if i > 0 and input_words[i - 1] != \"at\":\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def normalize(score, alpha=15):\n",
    "    \"\"\"\n",
    "    Normalize the score to be between -1 and 1 using an alpha that\n",
    "    approximates the max expected value\n",
    "    \"\"\"\n",
    "    norm_score = score / math.sqrt((score * score) + alpha)\n",
    "    if norm_score < -1.0:\n",
    "        return -1.0\n",
    "    elif norm_score > 1.0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return norm_score\n",
    "    \n",
    "    \n",
    "def allcap_differential(words):\n",
    "    \"\"\"\n",
    "    Check whether just some words in the input are ALL CAPS\n",
    "    :param list words: The words to inspect\n",
    "    :returns: `True` if some but not all items in `words` are ALL CAPS\n",
    "    \"\"\"\n",
    "    is_different = False\n",
    "    allcap_words = 0\n",
    "    for word in words:\n",
    "        if word.isupper():\n",
    "            allcap_words += 1\n",
    "    cap_differential = len(words) - allcap_words\n",
    "    if 0 < cap_differential < len(words):\n",
    "        is_different = True\n",
    "    return is_different\n",
    "\n",
    "\n",
    "def scalar_inc_dec(word, valence, is_cap_diff):\n",
    "    \"\"\"\n",
    "    Check if the preceding words increase, decrease, or negate/nullify the\n",
    "    valence\n",
    "    \"\"\"\n",
    "    scalar = 0.0\n",
    "    word_lower = word.lower()\n",
    "    if word_lower in BOOSTER_DICT:\n",
    "        scalar = BOOSTER_DICT[word_lower]\n",
    "        if valence < 0:\n",
    "            scalar *= -1\n",
    "        # check if booster/dampener word is in ALLCAPS (while others aren't)\n",
    "        if word.isupper() and is_cap_diff:\n",
    "            if valence > 0:\n",
    "                scalar += C_INCR\n",
    "            else:\n",
    "                scalar -= C_INCR\n",
    "    return scalar\n",
    "\n",
    "\n",
    "class SentiText(object):\n",
    "    \"\"\"\n",
    "    Identify sentiment-relevant string-level properties of input text.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text):\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text).encode('utf-8')\n",
    "        self.text = text\n",
    "        self.words_and_emoticons = self._words_and_emoticons()\n",
    "        # doesn't separate words from\\\n",
    "        # adjacent punctuation (keeps emoticons & contractions)\n",
    "        self.is_cap_diff = allcap_differential(self.words_and_emoticons)\n",
    "\n",
    "    def _words_plus_punc(self):\n",
    "        \"\"\"\n",
    "        Returns mapping of form:\n",
    "        {\n",
    "            'cat,': 'cat',\n",
    "            ',cat': 'cat',\n",
    "        }\n",
    "        \"\"\"\n",
    "        no_punc_text = REGEX_REMOVE_PUNCTUATION.sub('', self.text)\n",
    "        # removes punctuation (but loses emoticons & contractions)\n",
    "        words_only = no_punc_text.split()\n",
    "        # remove singletons\n",
    "        words_only = set(w for w in words_only if len(w) > 1)\n",
    "        # the product gives ('cat', ',') and (',', 'cat')\n",
    "        punc_before = {''.join(p): p[1] for p in product(PUNC_LIST, words_only)}\n",
    "        punc_after = {''.join(p): p[0] for p in product(words_only, PUNC_LIST)}\n",
    "        words_punc_dict = punc_before\n",
    "        words_punc_dict.update(punc_after)\n",
    "        return words_punc_dict\n",
    "\n",
    "    def _words_and_emoticons(self):\n",
    "        \"\"\"\n",
    "        Removes leading and trailing puncutation\n",
    "        Leaves contractions and most emoticons\n",
    "            Does not preserve punc-plus-letter emoticons (e.g. :D)\n",
    "        \"\"\"\n",
    "        wes = self.text.split()\n",
    "        words_punc_dict = self._words_plus_punc()\n",
    "        wes = [we for we in wes if len(we) > 1]\n",
    "        for i, we in enumerate(wes):\n",
    "            if we in words_punc_dict:\n",
    "                wes[i] = words_punc_dict[we]\n",
    "        return wes\n",
    "\n",
    "\n",
    "class SentimentIntensityAnalyzer(object):\n",
    "    \"\"\"\n",
    "    Give a sentiment intensity score to sentences.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lexicon_file=\"vader_lexicon.txt\", emoji_lexicon=\"emoji_utf8_lexicon.txt\"):\n",
    "        _this_module_file_path_ = os.path.abspath(getsourcefile(lambda: 0))\n",
    "        lexicon_full_filepath = os.path.join(os.path.dirname(_this_module_file_path_), lexicon_file)\n",
    "        with open(lexicon_full_filepath, encoding='utf-8') as f:\n",
    "            self.lexicon_full_filepath = f.read()\n",
    "        self.lexicon = self.make_lex_dict()\n",
    "\n",
    "        emoji_full_filepath = os.path.join(os.path.dirname(_this_module_file_path_), emoji_lexicon)\n",
    "        with open(emoji_full_filepath, encoding='utf-8') as f:\n",
    "            self.emoji_full_filepath = f.read()\n",
    "        self.emojis = self.make_emoji_dict()\n",
    "\n",
    "    def make_lex_dict(self):\n",
    "        \"\"\"\n",
    "        Convert lexicon file to a dictionary\n",
    "        \"\"\"\n",
    "        lex_dict = {}\n",
    "        for line in self.lexicon_full_filepath.split('\\n'):\n",
    "            (word, measure) = line.strip().split('\\t')[0:2]\n",
    "            lex_dict[word] = float(measure)\n",
    "        return lex_dict\n",
    "\n",
    "    def make_emoji_dict(self):\n",
    "        \"\"\"\n",
    "        Convert emoji lexicon file to a dictionary\n",
    "        \"\"\"\n",
    "        emoji_dict = {}\n",
    "        for line in self.emoji_full_filepath.split('\\n'):\n",
    "            (emoji, description) = line.strip().split('\\t')[0:2]\n",
    "            emoji_dict[emoji] = description\n",
    "        return emoji_dict\n",
    "\n",
    "    def polarity_scores(self, text):\n",
    "        \"\"\"\n",
    "        Return a float for sentiment strength based on the input text.\n",
    "        Positive values are positive valence, negative value are negative\n",
    "        valence.\n",
    "        \"\"\"\n",
    "        # convert emojis to their textual descriptions\n",
    "        text_token_list = text.split()\n",
    "        text_no_emoji_lst = []\n",
    "        for token in text_token_list:\n",
    "            if token in self.emojis:\n",
    "                # get the textual description\n",
    "                description = self.emojis[token]\n",
    "                text_no_emoji_lst.append(description)\n",
    "            else:\n",
    "                text_no_emoji_lst.append(token)\n",
    "        text = \" \".join(x for x in text_no_emoji_lst)\n",
    "\n",
    "        sentitext = SentiText(text)\n",
    "\n",
    "        sentiments = []\n",
    "        words_and_emoticons = sentitext.words_and_emoticons\n",
    "        for item in words_and_emoticons:\n",
    "            valence = 0\n",
    "            i = words_and_emoticons.index(item)\n",
    "            # check for vader_lexicon words that may be used as modifiers or negations\n",
    "            if item.lower() in BOOSTER_DICT:\n",
    "                sentiments.append(valence)\n",
    "                continue\n",
    "            if (i < len(words_and_emoticons) - 1 and item.lower() == \"kind\" and\n",
    "                    words_and_emoticons[i + 1].lower() == \"of\"):\n",
    "                sentiments.append(valence)\n",
    "                continue\n",
    "\n",
    "            sentiments = self.sentiment_valence(valence, sentitext, item, i, sentiments)\n",
    "\n",
    "        sentiments = self._but_check(words_and_emoticons, sentiments)\n",
    "\n",
    "        valence_dict = self.score_valence(sentiments, text)\n",
    "\n",
    "        return valence_dict\n",
    "\n",
    "    def sentiment_valence(self, valence, sentitext, item, i, sentiments):\n",
    "        is_cap_diff = sentitext.is_cap_diff\n",
    "        words_and_emoticons = sentitext.words_and_emoticons\n",
    "        item_lowercase = item.lower()\n",
    "        if item_lowercase in self.lexicon:\n",
    "            # get the sentiment valence\n",
    "            valence = self.lexicon[item_lowercase]\n",
    "            # check if sentiment laden word is in ALL CAPS (while others aren't)\n",
    "            if item.isupper() and is_cap_diff:\n",
    "                if valence > 0:\n",
    "                    valence += C_INCR\n",
    "                else:\n",
    "                    valence -= C_INCR\n",
    "\n",
    "            for start_i in range(0, 3):\n",
    "                # dampen the scalar modifier of preceding words and emoticons\n",
    "                # (excluding the ones that immediately preceed the item) based\n",
    "                # on their distance from the current item.\n",
    "                if i > start_i and words_and_emoticons[i - (start_i + 1)].lower() not in self.lexicon:\n",
    "                    s = scalar_inc_dec(words_and_emoticons[i - (start_i + 1)], valence, is_cap_diff)\n",
    "                    if start_i == 1 and s != 0:\n",
    "                        s = s * 0.95\n",
    "                    if start_i == 2 and s != 0:\n",
    "                        s = s * 0.9\n",
    "                    valence = valence + s\n",
    "                    valence = self._negation_check(valence, words_and_emoticons, start_i, i)\n",
    "                    if start_i == 2:\n",
    "                        valence = self._special_idioms_check(valence, words_and_emoticons, i)\n",
    "\n",
    "            valence = self._least_check(valence, words_and_emoticons, i)\n",
    "        sentiments.append(valence)\n",
    "        return sentiments\n",
    "\n",
    "    def _least_check(self, valence, words_and_emoticons, i):\n",
    "        # check for negation case using \"least\"\n",
    "        if i > 1 and words_and_emoticons[i - 1].lower() not in self.lexicon \\\n",
    "                and words_and_emoticons[i - 1].lower() == \"least\":\n",
    "            if words_and_emoticons[i - 2].lower() != \"at\" and words_and_emoticons[i - 2].lower() != \"very\":\n",
    "                valence = valence * N_SCALAR\n",
    "        elif i > 0 and words_and_emoticons[i - 1].lower() not in self.lexicon \\\n",
    "                and words_and_emoticons[i - 1].lower() == \"least\":\n",
    "            valence = valence * N_SCALAR\n",
    "        return valence\n",
    "\n",
    "    @staticmethod\n",
    "    def _but_check(words_and_emoticons, sentiments):\n",
    "        # check for modification in sentiment due to contrastive conjunction 'but'\n",
    "        words_and_emoticons_lower = [str(w).lower() for w in words_and_emoticons]\n",
    "        if 'but' in words_and_emoticons_lower:\n",
    "            bi = words_and_emoticons_lower.index('but')\n",
    "            for sentiment in sentiments:\n",
    "                si = sentiments.index(sentiment)\n",
    "                if si < bi:\n",
    "                    sentiments.pop(si)\n",
    "                    sentiments.insert(si, sentiment * 0.5)\n",
    "                elif si > bi:\n",
    "                    sentiments.pop(si)\n",
    "                    sentiments.insert(si, sentiment * 1.5)\n",
    "        return sentiments\n",
    "\n",
    "    @staticmethod\n",
    "    def _special_idioms_check(valence, words_and_emoticons, i):\n",
    "        words_and_emoticons_lower = [str(w).lower() for w in words_and_emoticons]\n",
    "        onezero = \"{0} {1}\".format(words_and_emoticons_lower[i - 1], words_and_emoticons_lower[i])\n",
    "\n",
    "        twoonezero = \"{0} {1} {2}\".format(words_and_emoticons_lower[i - 2],\n",
    "                                          words_and_emoticons_lower[i - 1], words_and_emoticons_lower[i])\n",
    "\n",
    "        twoone = \"{0} {1}\".format(words_and_emoticons_lower[i - 2], words_and_emoticons_lower[i - 1])\n",
    "\n",
    "        threetwoone = \"{0} {1} {2}\".format(words_and_emoticons_lower[i - 3],\n",
    "                                           words_and_emoticons_lower[i - 2], words_and_emoticons_lower[i - 1])\n",
    "\n",
    "        threetwo = \"{0} {1}\".format(words_and_emoticons_lower[i - 3], words_and_emoticons_lower[i - 2])\n",
    "\n",
    "        sequences = [onezero, twoonezero, twoone, threetwoone, threetwo]\n",
    "\n",
    "        for seq in sequences:\n",
    "            if seq in SPECIAL_CASE_IDIOMS:\n",
    "                valence = SPECIAL_CASE_IDIOMS[seq]\n",
    "                break\n",
    "\n",
    "        if len(words_and_emoticons_lower) - 1 > i:\n",
    "            zeroone = \"{0} {1}\".format(words_and_emoticons_lower[i], words_and_emoticons_lower[i + 1])\n",
    "            if zeroone in SPECIAL_CASE_IDIOMS:\n",
    "                valence = SPECIAL_CASE_IDIOMS[zeroone]\n",
    "        if len(words_and_emoticons_lower) - 1 > i + 1:\n",
    "            zeroonetwo = \"{0} {1} {2}\".format(words_and_emoticons_lower[i], words_and_emoticons_lower[i + 1],\n",
    "                                              words_and_emoticons_lower[i + 2])\n",
    "            if zeroonetwo in SPECIAL_CASE_IDIOMS:\n",
    "                valence = SPECIAL_CASE_IDIOMS[zeroonetwo]\n",
    "\n",
    "        # check for booster/dampener bi-grams such as 'sort of' or 'kind of'\n",
    "        n_grams = [threetwoone, threetwo, twoone]\n",
    "        for n_gram in n_grams:\n",
    "            if n_gram in BOOSTER_DICT:\n",
    "                valence = valence + BOOSTER_DICT[n_gram]\n",
    "        return valence\n",
    "\n",
    "    @staticmethod\n",
    "    def _sentiment_laden_idioms_check(valence, senti_text_lower):\n",
    "        # Future Work\n",
    "        # check for sentiment laden idioms that don't contain a lexicon word\n",
    "        idioms_valences = []\n",
    "        for idiom in SENTIMENT_LADEN_IDIOMS:\n",
    "            if idiom in senti_text_lower:\n",
    "                print(idiom, senti_text_lower)\n",
    "                valence = SENTIMENT_LADEN_IDIOMS[idiom]\n",
    "                idioms_valences.append(valence)\n",
    "        if len(idioms_valences) > 0:\n",
    "            valence = sum(idioms_valences) / float(len(idioms_valences))\n",
    "        return valence\n",
    "\n",
    "    @staticmethod\n",
    "    def _negation_check(valence, words_and_emoticons, start_i, i):\n",
    "        words_and_emoticons_lower = [str(w).lower() for w in words_and_emoticons]\n",
    "        if start_i == 0:\n",
    "            if negated([words_and_emoticons_lower[i - (start_i + 1)]]):  # 1 word preceding lexicon word (w/o stopwords)\n",
    "                valence = valence * N_SCALAR\n",
    "        if start_i == 1:\n",
    "            if words_and_emoticons_lower[i - 2] == \"never\" and \\\n",
    "                    (words_and_emoticons_lower[i - 1] == \"so\" or\n",
    "                     words_and_emoticons_lower[i - 1] == \"this\"):\n",
    "                valence = valence * 1.25\n",
    "            elif words_and_emoticons_lower[i - 2] == \"without\" and \\\n",
    "                    words_and_emoticons_lower[i - 1] == \"doubt\":\n",
    "                valence = valence\n",
    "            elif negated([words_and_emoticons_lower[i - (start_i + 1)]]):  # 2 words preceding the lexicon word position\n",
    "                valence = valence * N_SCALAR\n",
    "        if start_i == 2:\n",
    "            if words_and_emoticons_lower[i - 3] == \"never\" and \\\n",
    "                    (words_and_emoticons_lower[i - 2] == \"so\" or words_and_emoticons_lower[i - 2] == \"this\") or \\\n",
    "                    (words_and_emoticons_lower[i - 1] == \"so\" or words_and_emoticons_lower[i - 1] == \"this\"):\n",
    "                valence = valence * 1.25\n",
    "            elif words_and_emoticons_lower[i - 3] == \"without\" and \\\n",
    "                    (words_and_emoticons_lower[i - 2] == \"doubt\" or words_and_emoticons_lower[i - 1] == \"doubt\"):\n",
    "                valence = valence\n",
    "            elif negated([words_and_emoticons_lower[i - (start_i + 1)]]):  # 3 words preceding the lexicon word position\n",
    "                valence = valence * N_SCALAR\n",
    "        return valence\n",
    "\n",
    "    def _punctuation_emphasis(self, text):\n",
    "        # add emphasis from exclamation points and question marks\n",
    "        ep_amplifier = self._amplify_ep(text)\n",
    "        qm_amplifier = self._amplify_qm(text)\n",
    "        punct_emph_amplifier = ep_amplifier + qm_amplifier\n",
    "        return punct_emph_amplifier\n",
    "\n",
    "    @staticmethod\n",
    "    def _amplify_ep(text):\n",
    "        # check for added emphasis resulting from exclamation points (up to 4 of them)\n",
    "        ep_count = text.count(\"!\")\n",
    "        if ep_count > 4:\n",
    "            ep_count = 4\n",
    "        # (empirically derived mean sentiment intensity rating increase for\n",
    "        # exclamation points)\n",
    "        ep_amplifier = ep_count * 0.292\n",
    "        return ep_amplifier\n",
    "\n",
    "    @staticmethod\n",
    "    def _amplify_qm(text):\n",
    "        # check for added emphasis resulting from question marks (2 or 3+)\n",
    "        qm_count = text.count(\"?\")\n",
    "        qm_amplifier = 0\n",
    "        if qm_count > 1:\n",
    "            if qm_count <= 3:\n",
    "                # (empirically derived mean sentiment intensity rating increase for\n",
    "                # question marks)\n",
    "                qm_amplifier = qm_count * 0.18\n",
    "            else:\n",
    "                qm_amplifier = 0.96\n",
    "        return qm_amplifier\n",
    "\n",
    "    @staticmethod\n",
    "    def _sift_sentiment_scores(sentiments):\n",
    "        # want separate positive versus negative sentiment scores\n",
    "        pos_sum = 0.0\n",
    "        neg_sum = 0.0\n",
    "        neu_count = 0\n",
    "        for sentiment_score in sentiments:\n",
    "            if sentiment_score > 0:\n",
    "                pos_sum += (float(sentiment_score) + 1)  # compensates for neutral words that are counted as 1\n",
    "            if sentiment_score < 0:\n",
    "                neg_sum += (float(sentiment_score) - 1)  # when used with math.fabs(), compensates for neutrals\n",
    "            if sentiment_score == 0:\n",
    "                neu_count += 1\n",
    "        return pos_sum, neg_sum, neu_count\n",
    "\n",
    "    def score_valence(self, sentiments, text):\n",
    "        if sentiments:\n",
    "            sum_s = float(sum(sentiments))\n",
    "            # compute and add emphasis from punctuation in text\n",
    "            punct_emph_amplifier = self._punctuation_emphasis(text)\n",
    "            if sum_s > 0:\n",
    "                sum_s += punct_emph_amplifier\n",
    "            elif sum_s < 0:\n",
    "                sum_s -= punct_emph_amplifier\n",
    "\n",
    "            compound = normalize(sum_s)\n",
    "            # discriminate between positive, negative and neutral sentiment scores\n",
    "            pos_sum, neg_sum, neu_count = self._sift_sentiment_scores(sentiments)\n",
    "\n",
    "            if pos_sum > math.fabs(neg_sum):\n",
    "                pos_sum += punct_emph_amplifier\n",
    "            elif pos_sum < math.fabs(neg_sum):\n",
    "                neg_sum -= punct_emph_amplifier\n",
    "\n",
    "            total = pos_sum + math.fabs(neg_sum) + neu_count\n",
    "            pos = math.fabs(pos_sum / total)\n",
    "            neg = math.fabs(neg_sum / total)\n",
    "            neu = math.fabs(neu_count / total)\n",
    "\n",
    "        else:\n",
    "            compound = 0.0\n",
    "            pos = 0.0\n",
    "            neg = 0.0\n",
    "            neu = 0.0\n",
    "\n",
    "        sentiment_dict = \\\n",
    "            {\"neg\": round(neg, 3),\n",
    "             \"neu\": round(neu, 3),\n",
    "             \"pos\": round(pos, 3),\n",
    "             \"compound\": round(compound, 4)}\n",
    "\n",
    "        return sentiment_dict\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changes made to vaderSentiment\n",
    "\n",
    "### Add new words from LIWC postitive/negative emotional 2015 dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_words = {\n",
    "    'fabulously': 2.4,\n",
    "    'amazingly': 2.8,\n",
    "    'fairer': 1.3,\n",
    "    'fairest': 1.3,\n",
    "    'fantastically': 2.0,\n",
    "    'fantasy': 2.6,\n",
    "    'kindn': 2.2,\n",
    "    'yum': 2.4,\n",
    "    'benefitt': 1.6,\n",
    "    'bestest': 3.2,\n",
    "    'bestie': 3.2,\n",
    "    'besties': 3.2,\n",
    "    'finer': 0.8,\n",
    "    'finest': 0.8,\n",
    "    'fortunately': 1.9,\n",
    "    'free-think': 1.0,\n",
    "    'freethink': 1.0,\n",
    "    'lovelier': 2.8,\n",
    "    'loveliest': 2.8,\n",
    "    'grande': 1.0,\n",
    "    'greatness': 3.2,\n",
    "    'okey': 0.9,\n",
    "    'peacekeep': 1.6,\n",
    "    'peacemak': 2.0,\n",
    "    'sweeter': 2.0,\n",
    "    'sweetest': 2.0,\n",
    "    'tender': 0.5,\n",
    "    'thanking': 1.8,\n",
    "    'thanx': 1.9,\n",
    "    'thnx': 1.9,\n",
    "    'heartwarm': 2.1,\n",
    "    \"hero's\": 2.6,\n",
    "    'enemie': -2.2,\n",
    "    'ineffect': -1.3,\n",
    "    'poorly': -2.1,\n",
    "    'poorness': -2.1,\n",
    "    'uncontrol': -1.5,\n",
    "    'asshole': -2.8,\n",
    "    'fuckh': -2.5,\n",
    "    'fuckin': -2.5,\n",
    "    'fucktwat': -3.1,\n",
    "    'fuckwad': -3.1,\n",
    "    'messier': -1.5,\n",
    "    'scarier': -2.2,\n",
    "    'scariest': -2.2,\n",
    "    'deceptive':-1.9,\n",
    "    'defend': -0.2,\n",
    "    'harmful': -2.6,\n",
    "    'messier': -1.5,\n",
    "    'messiest': -1.5,\n",
    "    'strangest': -0.8,\n",
    "    'wrongdoing': -1.9,\n",
    "    'wrongful': -1.9,\n",
    "    'wrongly': -1.9,\n",
    "    'wrongness;': -1.9,\n",
    "    'wrongs': -1.9,\n",
    "    'heartbroke': -2.7,\n",
    "    'petrif': -0.3,\n",
    "    'traged': -2.7\n",
    "}\n",
    "\n",
    "analyzer.lexicon.update(new_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redefine Cleaning Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import HTMLParser\n",
    "html_parser = HTMLParser.HTMLParser()\n",
    "import re\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "split_adhere_dic = { # split\n",
    "                    \"cause\": \"because\", \"could've\": \"could have\", \n",
    "                    \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \n",
    "                    \"he'll've\": \"he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
    "                    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \n",
    "                    \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n",
    "                    \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \n",
    "                    \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \n",
    "                    \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\",\n",
    "                    \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \n",
    "                    \"it'll've\": \"it will have\",\"it's\": \"it is\", \n",
    "                    \"let's\": \"let us\", \"ma'am\": \"madam\", \"might've\": \"might have\",\n",
    "                    \"o'clock\": \"of the clock\", \n",
    "                    \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \n",
    "                    \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\",\n",
    "                    \"so've\": \"so have\",\"so's\": \"so as\", \n",
    "                    \"this's\": \"this is\",\n",
    "                    \"that'd\": \"that would\", \"that'd've\": \"that would have\",\"that's\": \"that is\", \n",
    "                    \"there'd\": \"there would\", \"there'd've\": \"there would have\",\"there's\": \"there is\", \n",
    "                    \"here's\": \"here is\",\n",
    "                    \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n",
    "                    \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n",
    "                    \"to've\": \"to have\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \n",
    "                    \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\",\n",
    "                    \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \n",
    "                    \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n",
    "                    \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \n",
    "                    \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \n",
    "                    \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \n",
    "                    \"why've\": \"why have\", \"will've\": \"will have\", \"would've\": \"would have\",\n",
    "                    \"y'all\": \"you all\", \"y'all'd\": \"you all would\", \"y'all'd've\": \"you all would have\",\n",
    "                    \"y'all're\": \"you all are\",\"y'all've\": \"you all have\", \"you'd\": \"you would\", \n",
    "                    \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n",
    "                    \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\",\n",
    "                     # adhere \n",
    "                    \"is not\": \"ain't\", \"are not\": \"aren't\",\"cannot\": \"can't\", \"can not\": \"can't\", \n",
    "                    \"cannot have\": \"can't've\", \"could not\": \"couldn't\", \"could not have\": \"couldn't've\",\"did not\": \"didn't\", \n",
    "                    \"does not\": \"doesn't\", \"do not\": \"don't\", \"had not\": \"hadn't\",  \n",
    "                    \"had not have\": \"hadn't've\", \"has not\": \"hasn't\", \"have not\": \"haven't\", \"is not\": \"isn't\", \n",
    "                    \"may not\": \"mayn't\", \"might not\": \"mightn't\",\"might not have\": \"mightn't've\", \n",
    "                    \"must have\": \"must've\", \"must not\": \"mustn't\", \"must not have\": \"mustn't've\", \n",
    "                    \"need not\": \"needn't\", \"need not have\": \"needn't've\",\n",
    "                    \"ought not\": \"oughtn't\", \"ought not have\": \"oughtn't've\", \"shall not\": \"shan't\",\n",
    "                    \"shall not\": \"sha'n't\", \"shall not have\": \"shan't've\", \n",
    "                    \"should not\": \"shouldn't\", \"should not have\": \"shouldn't've\", \"was not\": \"wasn't\", \"were not\": \"weren't\",\n",
    "                    \"will not\": \"won't\", \"will not have\": \"won't've\",  \"would not\": \"wouldn't\", \n",
    "                    \"would not have\": \"wouldn't've\", \n",
    "                    \"no prblme\": \"noproblem\", \"no prob\": \"noprob\", \"no problems\": \"noproblmes\", \"no probs\": \"noprobs\",\n",
    "                    \"no worry\": \"noworry\", \"no worries\": \"noworries\", \"good bey\": \"goodbey\", \"long time no see\": \"longtimenosee\", \n",
    "                    \"nothing but\": \"nothingbut\", \"no more\": \"nomore\",\n",
    "                    \"kind of\": \"kindof\", \"sort of\": \"sortof\"}\n",
    "\n",
    "pat1 = r'@[\\w_]+' # @-mention\n",
    "pat2 = r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+' # URLs\n",
    "pat5 = r'www.[^ ]+' # additions to URLs, texts with 'www..'\n",
    "combined_pat = r'|'.join((pat1, pat2, pat5))\n",
    "\n",
    "split_pattern = re.compile(r'\\b(' + '|'.join(split_adhere_dic.keys()) + r')\\b')\n",
    "\n",
    "def tweet_cleaner(demo):\n",
    "    soup = BeautifulSoup(demo, 'lxml') # HTML\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    split_handled = split_pattern.sub(lambda x: split_adhere_dic[x.group()], stripped)\n",
    "\n",
    "    return split_handled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the Updated Analyzer to Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_NLTKCorpus_Twitter_clean.csv', index_col = 0, encoding = \"ISO-8859-1\")\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "df['text'] = [tweet_cleaner(t) for t in df.clean_text]\n",
    "df.drop(columns = ['clean_text'], inplace=True)\n",
    "df.to_csv('train_NLTKCorpus_Twitter_clean_new.csv', encoding = 'utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compound range from (-0.35, 0.35) | neu >= 0.55 Accuracy Score : 92.39%\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('train_NLTKCorpus_Twitter_clean_new.csv')\n",
    "df.dropna(inplace=True)\n",
    "vader = df['text'].apply(lambda x : analyzer.polarity_scores(x))\n",
    "df=pd.concat([df,vader.apply(pd.Series)],1)\n",
    "\n",
    "df['com'] = df['compound']\n",
    "df = df.drop(df[(df['compound'] >= -0.35) & (df['compound'] <= 0.35)].index)\n",
    "df.loc[df['compound'] > 0.35, 'compound'] = 1\n",
    "df.loc[df['compound'] < -0.35, 'compound'] = 0\n",
    "df = df.drop(df[df['neu'] >=0.55].index)\n",
    "df['cheating0.35'] = np.where(df['target'] != df['compound'], 'yes', 'no')\n",
    "\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "print \"Compound range from (-0.35, 0.35) | neu >= 0.55 Accuracy Score : {0:.2f}%\".format((1 - (len(df.loc[df['cheating0.35'] == 'yes']) / len(df)))*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
